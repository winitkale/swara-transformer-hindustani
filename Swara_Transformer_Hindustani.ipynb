{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swara Transformer â€“ Hindustani Classical Music"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Sample demo corpus\n",
    "raga_data = {\n",
    "    'Raga_Yaman': [\"S R G M D N S'\", \"S' N D P M G R S\"],\n",
    "    'Raga_Bhairav': [\"S r G M P d N S'\", \"S' N d P M G r S\"]\n",
    "}\n",
    "\n",
    "# Flatten corpus with raga label\n",
    "lines = []\n",
    "for raga, phrases in raga_data.items():\n",
    "    for phrase in phrases:\n",
    "        lines.append(f'|{raga}| ' + phrase)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['|Raga_Yaman|', '|Raga_Bhairav|', 'S', 'R', 'G', 'M', 'P', 'D', 'N', \"S'\"]})\n",
    "encodings = tokenizer('\n'.join(lines), return_tensors='pt')\n",
    "\n",
    "# Dataset\n",
    "class SwaraDataset(Dataset):\n",
    "    def __init__(self, encodings, block_size=16):\n",
    "        self.input_ids = encodings['input_ids'][0]\n",
    "        self.block_size = block_size\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_ids[idx:idx+self.block_size]\n",
    "        y = self.input_ids[idx+1:idx+1+self.block_size]\n",
    "        return {'input_ids': x, 'labels': y}\n",
    "\n",
    "dataset = SwaraDataset(encodings)\n",
    "\n",
    "# Model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(output_dir='./results', per_device_train_batch_size=2, num_train_epochs=2, logging_steps=10, report_to='none')\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=dataset)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}